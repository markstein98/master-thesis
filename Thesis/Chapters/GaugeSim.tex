\pagestyle{myFancy}
\chapter{Computer Simulation of Pure Gauge Theories}
\section{Introduction}
Monte Carlo simulations are a powerful tool that can be used to evaluate observables in lattice field theory.
Given an observable $O$ (such as a plaquette, a Wilson or Polyakov loop, etc.), its vacuum expectation value is formally given by the functional integral
\begin{equation}
    \expval{O} = \frac1Z\int\DD[U]e^{-S[U]}O[U] \qquad \text{with} \qquad Z = \int\DD[U]e^{-S[U]}O[U] \label{2:Observable}
\end{equation}
where $\DD[U]$ is to be intended as a Haar measure.\\
This expression, however, cannot be evalued analytically except for very small lattices, therefore \eqref{2:Observable} is approximated by an average of the observable evaluated on $N$ sample gauge field configurations $\prc{U_n}$, distributed according a probability density $\varpropto e^{-S[U_n]}$.\\
The expectation value is then obtained by computing the following sum for a sufficient number of configurations generated by the proper Monte Carlo algorithm(s):
\begin{equation}
    \expval{O} \simeq \frac1N\sum_{\prc{U_n}}O[U_n] \label{2:ExpValObs}
\end{equation}
Because of the probability density $\varpropto e^{-S[U_n]}$, only configurations $\prc{U_n}$ that minimize the action are \emph{good configurations}, as all other configurations are exponentially suppressed.
For this reason, generating totally random gauge fields on the lattice links is not an efficient way to evaluate \eqref{2:ExpValObs}, as most of the $\prc{U_n}$ will have a very small Boltzmann factor (the $e^{-S[U_n]}$) and expression \eqref{2:ExpValObs} will give incorrect results unless a huge number (orders of magnitude higher than what is reasonably possible) of different configurations is tried.\\
In order to avoid the evaluation of a great number of configurations that would contribute little-to-nothing to the observables' values, a sequence of configurations $\prc{U_n}$ is generated through a Markov chain process, built such that its stationary distribution minimizes the action $S[U]$. This process is usually called \emph{thermalization}.\\
Every process of this type must begin from a starting configuration, usually chosen by the user.
The main starting configurations are usually two: if the simulation begins with the fields in an ordered way (for example, all the gauge links set to the identity), the initial configuration is called \emph{cold start}, if the simulation begins with random gauge fields in every link, the initial configuration is called \emph{hot start}.
Of course, the Markov chain must have the same stationary distribution if the hot or cold start is chosen.\\
In this chapter the main Monte Carlo algorithms used to thermalize the starting configuration are presented, then the method for evaluating some observables of interest, such as plaquettes and Polyakov loops is explained.
