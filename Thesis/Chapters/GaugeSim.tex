\pagestyle{myFancy}
\chapter{Computer Simulation of Pure Gauge Theories}
\section{Introduction}
Monte Carlo simulations are a powerful tool that can be used to evaluate observables in lattice field theory.
Given an observable $O$ (such as a plaquette, a Wilson or Polyakov loop, etc.), its vacuum expectation value is formally given by the functional integral
\begin{equation}
    \expval{O} = \frac1Z\int\DD[U]e^{-S[U]}O[U] \qquad \text{with} \qquad Z = \int\DD[U]e^{-S[U]}O[U] \label{2:Observable}
\end{equation}
where $\DD[U]$ is to be intended as a Haar measure.\\
This expression, however, cannot be evalued analytically except for very small lattices, therefore \eqref{2:Observable} is approximated by an average of the observable evaluated on $N$ sample gauge field configurations $\prc{U_n}$\footnote{Here the subscript $n$ distinguishes the different configurations.}, distributed according a probability density $\varpropto e^{-S[U_n]}$.\\
The expectation value is then obtained by computing the following sum for a sufficient number of configurations generated by the proper Monte Carlo algorithm(s):
\begin{equation}
    \expval{O} \simeq \frac1N\sum_{\prc{U_n}}O[U_n] \label{2:ExpValObs}
\end{equation}
Because of the probability density $\varpropto e^{-S[U_n]}$, only configurations $\prc{U_n}$ that minimize the action are \emph{good configurations}, as all other configurations are exponentially suppressed.
For this reason, generating totally random gauge fields on the lattice links is not an efficient way to evaluate \eqref{2:ExpValObs}, as most of the $\prc{U_n}$ will have a very small Boltzmann factor (the $e^{-S[U_n]}$) and expression \eqref{2:ExpValObs} will give incorrect results unless a huge number (orders of magnitude higher than what is reasonably possible) of different configurations is tried.\\
In order to avoid the evaluation of a great number of configurations that would contribute little-to-nothing to the observables' values, a sequence of configurations $\prc{U_n}$ is generated through a Markov chain process, built such that its stationary distribution minimizes the action $S[U]$. This process is usually called \emph{thermalization}.\\
Every process of this type must begin from a starting configuration, usually chosen by the user.
The main starting configurations are usually two: if the simulation begins with the fields in an ordered way (for example, all the gauge links set to the identity), the initial configuration is called \emph{cold start}, if the simulation begins with random gauge fields in every link, the initial configuration is called \emph{hot start}.
Of course, the Markov chain must have the same stationary distribution if the hot or cold start is chosen.\\
In this chapter the main Monte Carlo algorithms used to thermalize the starting configuration are presented, then the method for evaluating some observables of interest, such as plaquettes and Polyakov loops is explained.

\section{Markov Processes}
As anticipated before, a Markov chain is needed to evolve an arbitrary starting configuration $U_0$ up to a region of the configuration space with a relatively large Boltzmann factor $e^{-S}$, therefore with high probability.
A Markov process is charaterized by a probability of transitioning from a configuration $\alpha$ to another configuration $\beta$ depending only on $\alpha$ and $\beta$ and not on the history of the process, namely the previous transitions that already occourred.
In formulae, this is expressed as
\begin{equation}
    P(U_n=\beta|U_{n-1}=\alpha) = T(\beta|\alpha) \label{2:TransMatrix}
\end{equation}
That is to say that the transition matrix $T$ does not depend on the index $n$, representing the computer time.\\
Being a transition probability, the matrix $T$ must obey the following equations
\begin{align}
    0 \leq T(\beta|\alpha) \leq& 1 \qquad \forall \alpha,\beta \label{2:TPropProb} \\
    \sum_{\beta}T(\beta|\alpha) =& 1 \qquad \forall \alpha \label{2:TPropNorm}
\end{align}
where \eqref{2:TPropProb} is a consequence of $T(\beta|\alpha)$ representing a probability, and \eqref{2:TPropNorm} means that the probability of transitioning to any configuration must be $1$ (of course, the case when $\alpha=\beta$ is included as well).\\
In order for the stochastic process to not have any sink or source of probability, the following balance equation must be satisfied:
\begin{equation}
    \sum_\alpha T(\beta|\alpha)P(\alpha) = \sum_\alpha T(\alpha|\beta)P(\beta) \label{2:BalanceEq}
\end{equation}
This equation states that the probability of transitioning to the configuration $\beta$, written in the \lhs as the sum of the transition probability from the configuration $\alpha$ weighted by the probability $P(\alpha)$ that the system is actually in that configuration, must be equal to the probability of transitioning out of the configuration $\beta$, given by the probability of finding the system in the configuration $\beta$ times the transition probability $T(\alpha|\beta)$ over all the final configurations, in the right-hand side.
Thanks to \eqref{2:TPropNorm}, the \rhs is easily proven to be equal to $P(\beta)$.\\
A sufficient (but not necessary) condition to obey the balance equation \eqref{2:BalanceEq} is obtained by requiring that it holds true term-by-term, thus obtaining the detailed balance condition:
\begin{equation}
    T(\beta|\alpha)P(\alpha) = T(\alpha|\beta)P(\beta) \label{2:DetailedBalance}
\end{equation}
Although it is not a necessary condition, most algorithms, including the ones discussed in the following section, satisfy it.

\section{Monte Carlo Algorithms}
Monte Carlo algorithms are a class of algorithms that, singularly or combined together, allow to advance the Markov chain, while satisfying the condition presented in the previous section.
Each algorithm, if applied once, allows the transition from a configuration $U_{n-1}$ to a configuration $U_n$ (eventually the same as $U_{n-1}$). The repeated application of the algorithm allows to advance through the Markov chain.

\subsection{Metropolis Algorithm}
The first algorithm presented is the Metropolis algorithm. It is not very much efficient and usually it is not used in simulations, however it contains the fundamental steps that are present in some of the more advanced algorithms and it is quite easy to understand.
For this reasons it is ususally viewed as the \emph{``ancestor''} of all Monte Carlo algorithms and it is present in every textbook on the subject.\\
This algorithm consists in two steps that implement in one of the most simple ways the detailed balance condition \eqref{2:DetailedBalance}:
\begin{enumerate}[label=\arabic*)]
    \item A candidate configuration $\beta$ is chosen, according to some \emph{a priori} selection probability $T_0(\beta|\alpha)$, where $\alpha=U_{n-1}$.
    \item The candidate configuration $\beta$ is accepted as the new configuration $U_n$ with the acceptance probability
          \begin{equation}
              T_A(\beta|\alpha) = \min\pr{1,\frac{T_0(\alpha|\beta)\exp(-S[\beta])}{T_0(\beta|\alpha)\exp(-S[\alpha])}} \label{2:MetropolisAccProb}
          \end{equation}
          If it is not accepted, the unchanged configuration is considered again ($U_n=\alpha$) and the measurements are eventually made again.
\end{enumerate}
These two steps are repeated a sufficient amount of times up unitl the needed measurements are taken.\\
Note that the fact that $P(\alpha) = \frac{e^{-S[\alpha]}}{Z} \varpropto e^{-S[\alpha]}$ has been used.\\
The total transition probability $T$ is obtained through the product $T=T_0T_A$, as the two steps are independant from each other, and it is straightforward to see that it satisfies the detailed balance condition \eqref{2:DetailedBalance}:
\begin{align*}
    T(\beta|\alpha)P(\alpha) =& \frac1Z T_0(\beta|\alpha)T_A(\beta|\alpha)\exp(-S[\alpha]) = \\
    =& \frac1Z T_0(\beta|\alpha)\min\pr{1,\frac{T_0(\alpha|\beta)\exp(-S[\beta])}{T_0(\beta|\alpha)\exp(-S[\alpha])}}\exp(-S[\alpha]) = \\
    =& \frac1Z \min\pr{T_0(\beta|\alpha)\exp(-S[\alpha]), T_0(\alpha|\beta)\exp(-S[\beta])} = \\
    =& \frac1Z T(\alpha|\beta)\exp(-S[\beta]) = \\
    =& T(\alpha|\beta)P(\beta)
\end{align*}
\qed
In many cases a symmetric selection probability is used $T_0(\alpha|\beta)=T_0(\beta|\alpha)$, thus \eqref{2:MetropolisAccProb} simpliefies to:
\begin{equation}
    T_A(\beta|\alpha) = \min\pr{1, e^{-\Delta S}} \quad \text{with} \quad \Delta S = S[\beta]-S[\alpha] \label{2:MetropolisAccProbSymm}
\end{equation}
That means that if the new configuration lowers the action, the change is accepted with probability $1$ (as $e^{-\Delta S}>1$), otherwise it is accepted with a certain probability that decays exponentially as the difference in the action of the two configurations becomes greater.
This ensures that the algorithm \emph{moves across} the configuration space towards the minimums of the action, while allowing quantum fluctuations in order to not \emph{``get stuck''} on a local minimum.\\
If the change in the action is local (it involves a single link variable), $\Delta S$ can be computed using only the field values in the local neighbour. This will be the case for $\SUN$ gauge theories.

\subsubsection{Application to SU(N) Gauge Theories}
For a $\SUN$ gauge theory, the algorithm is implemented in the following way.
For each iteration, a single link is changed, then the acceptance probability is computed, a random number is extracted and, if it is less than the acceptance probability the change is accepted, otherwise it is rejceted. The algorithm is then iterated a certain number of times and measures are taken.\\
The candidate link $U'_\mu(x)$ for step 1 is generated in the vicinity of the old value $U_\mu(x)$, in order to not have a too great $\Delta S$ that would lead to too low accentance rates.
This can be done by exploiting the property that the product of any two elements of $\SUN$ is still an element of $\SUN$, therefore extracting a matrix $X\in\SUN$ near the identity allows to write the candidate link as:
\begin{equation}
    U'_\mu(x) = XU_\mu(x) \label{2:MetropolisCandidateLink}
\end{equation}
The matrix $X$ is chosen such that it has the same probability as $X^{-1}$, this way the selection probability $T_0$ is symmetric and the computation of the acceptance probability $T_A$ becomes easier.\\
For this reason, only the variation of the action $\Delta S$ must be computed, where of course the action is the Wilson action \eqref{1:WilsonAction}.
In particular, only the plaquettes containing the candidate link must be evaluated: the change in the action is local, so all the other plaquettes will have the same value both before and after the change of the link.
Hence $\Delta S = S[U'_\mu(x)]_{loc}-S[U_\mu(x)]_{loc}$.\\
In a SH lattice, each link is shared between $6$ plaquettes.
For each plaquette the change of the action is given by the change of the link, while the product of the other three gauge links, that is called \emph{staple} and will be indicated as $P_i$, remains unchanged.
Therefore, the local contribution to the action can be computed as:
\begin{equation*}
    S[U_\mu(x)]_{loc} = \frac\beta{2N} \sum_{i=1}^6 \Re\Tr\prs{\id-U_\mu(x)P_i} = \frac\beta{2N} \Re\Tr\prs{6\id-U_\mu(x)\sum_{i=1}^6P_i}
\end{equation*}
where the sum over all the staples is:
\begin{equation}
    A = \sum_{i=1}^6P_i = \sum_{\nu\neq\mu}\pr{U_\nu(x+\hat\mu)U^\dagger_\mu(x+\hat\nu)U^\dagger_\nu(x) + U^\dagger_\nu(x+\hat\mu)U^\dagger_\mu(x-\hat\nu)U_\nu(x-\hat\nu)} \label{2:SumOverStaples}
\end{equation}
The change of the action can now be computed as
\begin{equation}
    \Delta S = S[U'_\mu(x)]_{loc}-S[U_\mu(x)]_{loc} = \frac\beta{2N} \Re\Tr\prs{\pr{U_\mu(x)-U'_\mu(x)}A} \label{2:MetropolisActionVar}
\end{equation}
where $A$ is not affected by the change of $U_\mu(x)$.

\subsection{Heat Bath Algorithm}
The heat bath algorithm is an \emph{enhanced} version of the Metropolis algorithm, that combines the two steps into a single one and chooses the new candidate according to the probability distribution obtained by the computing the surrounding staples:
\begin{equation}
    \dd P(U) = \dd U \exp(\frac\beta{2N}\Re\Tr[U A]) \label{2:DistrProbHB}
\end{equation}
where $\dd U$ denotes the Haar integration measure of the gauge group and $A$ is computed according to \eqref{2:SumOverStaples}.
This probability distribution can be computationally quite demanding, but has the advantage that, unlike the Metropolis algorithm, the link variable always changes.\\
The implementation details depend on the gauge group, for this reason, the heat bath method for the gauge group $\SU(2)$ will be now explained\footnote{As matrixes of $\SUN$ can be built using $\SU(2)$ matrixes, the general case is just a little more complicated, but it follows the same principles.}.
Since the sum of any two $\SU(2)$ elements is proportional to another $\SU(2)$ element, the sum of all staples $A$ \eqref{2:SumOverStaples} can be written in the form
\begin{equation}
    A = V\sqrt{\det(A)} \qquad\text{with}\qquad V\in\SU(2) \label{2:SumOverStaplesSU2}
\end{equation}
where it can be proven that $\det(A)\geq0$.
Plugging into \eqref{2:DistrProbHB} and using the invariance of the Haar measure under trasformation of the origin of the group space ($\dd U = \dd (UV) = \dd X$), the distribution probability of the matrix $X=UV$ is
\begin{equation}
    \dd P(X) = \dd X \exp(\frac\beta{2N}\sqrt{\det(A)}\Re\Tr[X]) \label{2:DistrProbHBX}
\end{equation}
If a matrix $X$ distributed according to \eqref{2:DistrProbHBX} is generated, then the new candidate link, distributed according to \eqref{2:DistrProbHB}, is obtained as $U'_\mu(x) = XV^\dagger = \frac1{\sqrt{\det(A)}}XA^\dagger$.\\
Any $U\in\SU(2)$ matrix can be written in the following representation, using $4$ real numbers:
\begin{equation}
    U=x_0\id+\i \bm{x} \cdot \bm{\sigma} \qquad\text{with}\qquad \det(U)=\abs{x}^2=\sum_{i=0}^3x_i^2=1 \label{2:SU2Repr}
\end{equation}
where $\bm\sigma=\pr{\sigma_1,\sigma_2,\sigma_3}$ is a vector built using the Pauli matrices and $x=(x_0,\bm{x})$ can be seen as a normalized $4$-compnents vector.
Using this representation, the Haar measure in \eqref{2:DistrProbHBX} can be written as:
\begin{align*}
    \dd X =& \frac1{\pi^2} \dd^4x \delta\pr{x_0^2+\bm{x}-1} =\\
    =& \frac1{\pi^2} \dd^4x \frac{1}{2\sqrt{1-x_0^2}} \pr{\delta\pr{\abs{\bm{x}}-\sqrt{1-x_0^2}}+\delta\pr{\abs{\bm{x}}+\sqrt{1-x_0^2}}} \numthis\label{2:HaarMeasureHB}
\end{align*}
where a well known property of the Dirac delta function has been used.\\
The volume element can be rewritten in terms of the components of the vector $x$:
\begin{equation}
    \dd^4x = \dd x_0\dd\abs{x}\abs{x}^2\underbrace{\dd(\cos\theta)\dd\varphi}_{\dd^2\Omega} \label{2:VolumeElemParam}
\end{equation}
Plugging back into \eqref{2:HaarMeasureHB} and integrating out the $\abs{\bm{x}}$ thanks to the delta functions (actually, only the first one contributes, as $x_i^2\leq1$ $\forall i$), the Haar measure takes the form:
\begin{equation}
    \dd X = \frac1{\pi^2}\dd x_0\dd^2\Omega\frac{1-x_0^2}{2\sqrt{1-x_0^2}} = \frac1{2\pi^2} \dd x_0 \dd^2\Omega \sqrt{1-x_0^2} \label{2:HaarMeasureHBSimplified}
\end{equation}
Then, in terms of the variables, the probability distribution becomes:
\begin{equation}
    \dd P(X) = \frac1{2\pi^2} \dd x_0 \dd\cos\theta \dd\varphi \sqrt{1-x_0^2} \exp\pr{\frac\beta2 \sqrt{\det(A)}x_0} \label{2:DistrProbHBXSimplified}
\end{equation}
with $x_0\in[-1,1]$, $\cos\theta\in[-1,1]$, $\varphi\in[0,2\pi)$, where $\Tr(X)=2x_0$ has been used.\\
Thus, the problem of generating a matrix $X$ distributed according to \eqref{2:DistrProbHBX} has been reduced to the determination of three random variables $x_0$, $\theta$, $\varphi$, whose distribution factorizes, so they can be determined independetly from each other.
The random variable $x_0$, being distributed according to $\sqrt{1-x_0^2} \exp\pr{\frac\beta2 \sqrt{\det(A)}x_0}$, is determined through the auxiliary variable $\lambda\in[0,1]$ such that $x_0 = 1-2\lambda^2$, therefore
\begin{equation}
    \dd x_0\sqrt{1-x_0^2} \exp\pr{\frac\beta2 \sqrt{\det(A)}x_0} \varpropto \dd\lambda\lambda^2\sqrt{1-\lambda^2} \exp\pr{-\beta \sqrt{\det(A)}\lambda^2} \label{2:LambdaDistrHB}
\end{equation}
The variable $\lambda$ is generated with the polynomially modified Gaussian distribution density
\begin{equation}
    p_1(\lambda) = \lambda^2 e^{-\beta \sqrt{\det(A)}\lambda^2}
\end{equation}
and accepted with an accept/reject step using the square root function
\begin{equation}
    p_2(\lambda) = \sqrt{1-\lambda^2}
\end{equation}
There are several algorithms that can perform this computation in an efficient way~\cite{1998art, luscher1994portable}.\\
After this, the length $\abs{x} = \sqrt{1-x_0^2}$ is computed, in order to determine the remaining variables.\\
The variables $\cos\theta$ and $\varphi$ are uniformely distributed, therefore a possible way of proceeding is by generating three random uniformely distributed numbers $r_1$, $r_2$ and $r_3$ in the interval $[-1,1)$ and accepting them only if $r_1^2+r_2^2+r_3^2\leq1$.
Then, the $3$-vector $(r_1,r_2,r_3)$ is normalized to length $\abs{x}$ computed before, obtaining $\bm{x} = (x_1,x_2,x_3)$.\\
After these steps, the vector $x=(x_0,\bm{x})$ can be used to generate the matrix $X$ according to \eqref{2:DistrProbHBX}, using representation \eqref{2:SU2Repr}, thus the new link variable can be obtained.\\
This algorithm rapidly leads the Markov process to a minimum of the action, however the risk of ``getting stuck'' on a local minimum is present.
For this reason, it is usually combined with the overrelaxation algorithm discussed below.

\subsection{Overrelaxation Algorithm}
The overrelaxation algorithm tries to change the variables as much as possible, exploiting the property that new configuration are always accepted if they do not change the action, as can be seen in \eqref{2:MetropolisAccProbSymm}.
The case with gauge group $\Uem$ is the simplest one: the group element can be written as $U=e^{\i\varphi}$, the sum of staples becomes $A=\rho e^{\i\alpha}$ and the local action can be wirtten as:
\begin{equation}
    S[U]_{loc} = \frac\beta2 \Re(UA) = \frac\beta2 \rho \Re(e^{\i\varphi}e^{\i\alpha}) = \frac\beta2\rho\cos(\varphi+\alpha) \label{2:SlocORU1}  
\end{equation}
The reflection $\varphi+\alpha\to-\varphi-\alpha$ or the change $\varphi+\alpha\to2\pi-\varphi-2\alpha$ leave the local action invariant, thus a change of this type is always accepted.
For a non-Abelian group, this change is performed through the ansatz:
\begin{equation}
    U \to U' = V^\dagger U^\dagger V^\dagger \label{2:ORTransf}
\end{equation}
where $V$ is a gauge group element chosen such that the local action is invariant.
The selection probability for a transformation of this kind is symmetric, as can be easily proven by inverting \eqref{2:ORTransf}, obtaining $U = V^\dagger U'^\dagger V^\dagger$.\\
For the gauge group $\SU(2)$, the matrix $V$ is chosen proportional to the sum of staples: $V=\frac{A}{\sqrt{\det(A)}}$, like in \eqref{2:SumOverStaplesSU2}.
Because of this,
\begin{equation}
    \Tr(U'A) = \Tr(V^\dagger U^\dagger V^\dagger \sqrt{\det(A)} V) = \sqrt{\det(A)} \Tr(V^\dagger U^\dagger) = \Tr(A^\dagger U^\dagger) = \Tr(UA) \label{2:ORTrace}
\end{equation}
therefore the choice for $U'$ leaves the action invariant.
In case $\det(A)=0$, any random link variable is accepted.\\
As the overrelaxation algorithm leaves the action invariant, it moves the Markov chain in the subspace of constant action, called \emph{micro-canonical} ensemble, thus it is not ergodic: it does not visit every possible point in the configuration space if given enough time.
For this reason, it needs to be used in combination with other updating algorithms, such as some steps of the Metropolis or heat bath algorithm.

\section{Measurements}
In this section, the techiniques used to compute the value of some observables are explained.
After the proper Monte Carlo algorithms have been iterated a sufficient number of times, the lattice is \emph{thermalized} and measures can be taken.
The observables presented below are the plaquettes and Polyakov loops, because they are the ones that will be evaluated in the simulation in the fourth chapter.
%TODO: add reference to chapter 4

\subsection{Plaquettes}
The mean value of the plaquette, indicated as $\plaq$, is obtained by evaluating expression \eqref{1:Plaquette} for all possible plaquette in the lattice, summing all the results and taking the trace of the result, divided by the total number of plaquettes present in the lattice.
In formulae:
\begin{equation}
    \plaq = \frac{1}{12 N TL_xL_yL_z} \sum_{x\in\Lambda} \sum_{\mu=0}^3 \sum_{\substack{\nu=0 \\ \nu\neq\mu}}^3 \Re\Tr U_{\mu\nu}(x) \label{2:LatticePlaquette}
\end{equation}
where $N$ is the number of colors (the same $N$ in $\SUN$), $12=4\cdot3$ is the total number of ways of choosing the first direction $\mu$ times the number of possible ways of choosing the second direction $\nu$, and $TL_1L_2L_3$ is the number of sites of the lattice.\\
The expectation value of the plaquette is important because it is directly linked to the Wilson action, thus it gives an intuition of the thermalization of the lattice.

\subsection{Polyakov Loops}
Polyakov loops are another important observable that will be evaluated in the simulations.
In particular, the product of two Polyakov loops, that is a function of the static quark potential, will be used, as explained in \eqref{1:PolyakovPotential}.
In order to achieve better statistics, the translational invariance of the lattice is exploited, by computing the so-called \emph{nonzero momentum Polyakov correlators}.

\section{Brief Scheme of a Simulation}
